{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from twython import Twython\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import time\n",
    "import yaml\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_KEY = 'OumzA854PAnWwA8VNjwZ6O1T1'\n",
    "APP_SECRET = \"aHWN7hVHFYGTJIqLG6d1sUgT2tzelmB7LNxtQjJz0RkmLDIFGW\"\n",
    "\n",
    "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['music', 'jazz', 'rap', '(country music)', '(pop music)', '(rock music)', '(classical music)']\n",
    "keywords = pd.DataFrame(keywords)\n",
    "keywords.columns = [\"Keywords\"]\n",
    "keywords.to_csv(\"Keywords.csv\")\n",
    "keywords = ['music', 'jazz', 'rap', '(country music)', '(pop music)', '(rock music)', '(classical music)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = dt.datetime.now()\n",
    "cnt = 0\n",
    "tweetList = []\n",
    "# for keyword in keywords:\n",
    "tweets = twitter.cursor(twitter.search, q = '(classical music)' +' -filter:retweets', lang = 'en', count = 100, tweet_mode = \"extended\")\n",
    "for item in tweets:\n",
    "    if (cnt == 7000):\n",
    "        cnt = 0\n",
    "        tweetsDF = pd.DataFrame(tweetList)\n",
    "        tweetsDF.to_csv(\"tweets_collected_\"+str(date.day)+str(date.strftime(\"%m\"))+\"_\"+keyword+\".csv\")\n",
    "        tweetList = []\n",
    "        break\n",
    "    tweetList.append(item)\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF1 = pd.read_csv(\"Earlier collection/tweets_collected_1404_music.csv\", dtype={'id':object})\n",
    "tweetsDF1[\"keyword\"] = \"music\"\n",
    "tweetsDF2 = pd.read_csv(\"Earlier collection/tweets_collected_1404_jazz.csv\", dtype={'id':object})\n",
    "tweetsDF2[\"keyword\"] = \"jazz\"\n",
    "tweetsDF3 = pd.read_csv(\"Earlier collection/tweets_collected_1404_rap.csv\", dtype={'id':object})\n",
    "tweetsDF3[\"keyword\"] = \"rap\"\n",
    "tweetsDF4 = pd.read_csv(\"Earlier collection/tweets_collected_1404_(country music).csv\", dtype={'id':object})\n",
    "tweetsDF4[\"keyword\"] = \"country\"\n",
    "tweetsDF5 = pd.read_csv(\"Earlier collection/tweets_collected_1404_(pop music).csv\", dtype={'id':object})\n",
    "tweetsDF5[\"keyword\"] = \"pop\"\n",
    "tweetsDF6 = pd.read_csv(\"Earlier collection/tweets_collected_1404_(rock music).csv\", dtype={'id':object})\n",
    "tweetsDF6[\"keyword\"] = \"rock\"\n",
    "tweetsDF7 = pd.read_csv(\"Earlier collection/tweets_collected_1404_(classical music).csv\", dtype={'id':object})\n",
    "tweetsDF7[\"keyword\"] = \"classical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDFList = [tweetsDF1,tweetsDF2,tweetsDF3,tweetsDF4,tweetsDF5,tweetsDF6,tweetsDF7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = dt.datetime.now()\n",
    "finalTweetsDFList = []\n",
    "for tweetsDF in tweetsDFList:\n",
    "    cnt = 0\n",
    "    tempList = []\n",
    "    tweetList = []\n",
    "    date = dt.datetime.now()\n",
    "    for index, row in tweetsDF.iterrows():\n",
    "        tempList.append(row[\"id\"])\n",
    "        if cnt%100 == 0:\n",
    "            try:\n",
    "                tweetList.extend(twitter.lookup_status(id=tempList, tweet_mode = \"extended\"))\n",
    "                tempList = []\n",
    "            except:\n",
    "                print(\"Succesful till: \"+cnt)\n",
    "        cnt+=1\n",
    "    tweetList.extend(twitter.lookup_status(id=tempList, tweet_mode = \"extended\"))\n",
    "    keyword = tweetsDF[\"keyword\"][0]\n",
    "    tweetsDF = pd.DataFrame(tweetList)\n",
    "    tweetsDF[\"keyword\"] = keyword\n",
    "    finalTweetsDFList.append(tweetsDF)\n",
    "    tweetsDF.to_csv(\"Final Collection/tweets_recollected_\"+str(date.day)+str(date.strftime(\"%m\"))+\"_\"+keyword+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate tweets found!\n",
      "Total number of unique tweets:  45210\n"
     ]
    }
   ],
   "source": [
    "for tweetsDF in finalTweetsDFList:\n",
    "    if (tweetsDF.shape[0] != tweetsDF['id'].nunique()):\n",
    "        print(\"Duplicate tweets found!\")\n",
    "print(\"No duplicate tweets found!\")\n",
    "print(\"Total number of unique tweets: \", pd.concat(tweetsDFList)['id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:30<00:00,  4.34s/it]\n"
     ]
    }
   ],
   "source": [
    "stopWordsSet = set(stopwords.words('english')) \n",
    "stopWordsSet.add(\"u\")\n",
    "for tweetsDF in tqdm(finalTweetsDFList):\n",
    "    keyword = tweetsDF[\"keyword\"][0]\n",
    "    with open(\"Pre-Processed Tweets/tweets_text_\"+keyword+\".txt\", \"a+\") as text_file:\n",
    "        for index, row in tweetsDF.iterrows():\n",
    "            tweetText = str.strip(re.sub('\\s?https?://.*?[\\s|\\n]', '', row[\"full_text\"]+\" \"))\n",
    "            tweetText = str.strip(re.sub('@.*?[\\s|\\n]', '', tweetText+\" \"))\n",
    "            tweetText = re.sub('&amp;', '', tweetText)\n",
    "            tweetText = \" \".join(re.findall(\"[a-zA-Z]+\",tweetText))\n",
    "            tokenizedText = word_tokenize(tweetText.lower())\n",
    "            finalText = ''.join([WordNetLemmatizer().lemmatize(w) + \" \" for w in tokenizedText if not w in stopWordsSet and len(w) >= 3])\n",
    "            text_file.write(str.strip(finalText) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "topList = []\n",
    "word_count = [[1, \"dan\"], [3, \"fish\"], [2, \"man\"], [4, \"kiop\"]]\n",
    "for count, word in word_count:\n",
    "    if (len(topList) == 0):\n",
    "        topList.append([count, word])\n",
    "    else:\n",
    "        ind = 0\n",
    "        for boolVal in [count > wordCount for wordCount, word in topList]:\n",
    "            if (boolVal):\n",
    "                topList.insert(ind, [count, word])\n",
    "                if (len(topList) > 3):\n",
    "                    topList.pop(3)\n",
    "                break\n",
    "            ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 'kiop'], [3, 'fish'], [2, 'man']]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[False, False].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, True]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[count > wordCount for wordCount, word in topList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"reducer.py\"\"\"\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "topList = []\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if (len(topList) == 0):\n",
    "            topList.append([current_count, current_word])\n",
    "        else:\n",
    "            ind = 0\n",
    "            for boolVal in [current_count > wordCount for wordCount, word in topList]:\n",
    "                if (boolVal):\n",
    "                    topList.insert(ind, [current_count, current_word])\n",
    "                    if (len(topList) > 20):\n",
    "                        topList.pop(20)\n",
    "                    break\n",
    "                ind+=1\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    if (len(topList) == 0):\n",
    "        topList.append([current_count, current_word])\n",
    "    else:\n",
    "        ind = 0\n",
    "        for boolVal in [current_count > wordCount for wordCount, word in topList]:\n",
    "            if (boolVal):\n",
    "                topList.insert(ind, [current_count, current_word])\n",
    "                if (len(topList) > 20):\n",
    "                    topList.pop(20)\n",
    "                break\n",
    "            ind+=1\n",
    "    for count, word in topList:\n",
    "        print '%s\\t%s' % (word, count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
