{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from twython import Twython\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import time\n",
    "import yaml\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_KEY = 'OumzA854PAnWwA8VNjwZ6O1T1'\n",
    "APP_SECRET = \"aHWN7hVHFYGTJIqLG6d1sUgT2tzelmB7LNxtQjJz0RkmLDIFGW\"\n",
    "\n",
    "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['music', 'jazz', 'rap', '(country music)', '(pop music)', '(rock music)', '(classical music)']\n",
    "keywords = pd.DataFrame(keywords)\n",
    "keywords.columns = [\"Keywords\"]\n",
    "keywords.to_csv(\"Keywords.csv\")\n",
    "keywords = ['music', 'jazz', 'rap', '(country music)', '(pop music)', '(rock music)', '(classical music)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = dt.datetime.now()\n",
    "cnt = 0\n",
    "tweetList = []\n",
    "# for keyword in keywords:\n",
    "tweets = twitter.cursor(twitter.search, q = '(classical music)' +' -filter:retweets', lang = 'en', count = 100, tweet_mode = \"extended\")\n",
    "for item in tweets:\n",
    "    if (cnt == 7000):\n",
    "        cnt = 0\n",
    "        tweetsDF = pd.DataFrame(tweetList)\n",
    "        tweetsDF.to_csv(\"tweets_collected_\"+str(date.day)+str(date.strftime(\"%m\"))+\"_\"+keyword+\".csv\")\n",
    "        tweetList = []\n",
    "        break\n",
    "    tweetList.append(item)\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF1 = pd.read_csv(\"Earlier collection/tweets_collected_1404_music.csv\", dtype={'id':object})\n",
    "tweetsDF1[\"keyword\"] = \"music\"\n",
    "tweetsDF2 = pd.read_csv(\"Earlier collection/tweets_collected_1404_jazz.csv\", dtype={'id':object})\n",
    "tweetsDF2[\"keyword\"] = \"jazz\"\n",
    "tweetsDF3 = pd.read_csv(\"Earlier collection/tweets_collected_1404_rap.csv\", dtype={'id':object})\n",
    "tweetsDF3[\"keyword\"] = \"rap\"\n",
    "tweetsDF4 = pd.read_csv(\"Earlier collection/tweets_collected_1404_(country music).csv\", dtype={'id':object})\n",
    "tweetsDF4[\"keyword\"] = \"country\"\n",
    "tweetsDF5 = pd.read_csv(\"Earlier collection/tweets_collected_1404_(pop music).csv\", dtype={'id':object})\n",
    "tweetsDF5[\"keyword\"] = \"pop\"\n",
    "tweetsDF6 = pd.read_csv(\"Earlier collection/tweets_collected_1404_(rock music).csv\", dtype={'id':object})\n",
    "tweetsDF6[\"keyword\"] = \"rock\"\n",
    "tweetsDF7 = pd.read_csv(\"Earlier collection/tweets_collected_1404_(classical music).csv\", dtype={'id':object})\n",
    "tweetsDF7[\"keyword\"] = \"classical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDFList = [tweetsDF1,tweetsDF2,tweetsDF3,tweetsDF4,tweetsDF5,tweetsDF6,tweetsDF7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = dt.datetime.now()\n",
    "finalTweetsDFList = []\n",
    "for tweetsDF in tweetsDFList:\n",
    "    cnt = 0\n",
    "    tempList = []\n",
    "    tweetList = []\n",
    "    date = dt.datetime.now()\n",
    "    for index, row in tweetsDF.iterrows():\n",
    "        tempList.append(row[\"id\"])\n",
    "        if cnt%100 == 0:\n",
    "            try:\n",
    "                tweetList.extend(twitter.lookup_status(id=tempList, tweet_mode = \"extended\"))\n",
    "                tempList = []\n",
    "            except:\n",
    "                print(\"Succesful till: \"+cnt)\n",
    "        cnt+=1\n",
    "    tweetList.extend(twitter.lookup_status(id=tempList, tweet_mode = \"extended\"))\n",
    "    keyword = tweetsDF[\"keyword\"][0]\n",
    "    tweetsDF = pd.DataFrame(tweetList)\n",
    "    tweetsDF[\"keyword\"] = keyword\n",
    "    finalTweetsDFList.append(tweetsDF)\n",
    "    tweetsDF.to_csv(\"Final Collection/tweets_recollected_\"+str(date.day)+str(date.strftime(\"%m\"))+\"_\"+keyword+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['music', 'jazz', 'rap', 'country', 'pop', 'rock', 'classical']\n",
    "finalTweetsDFList = []\n",
    "for keyword in keywords:\n",
    "    finalTweetsDFList.append(pd.read_csv(\"Final Collection/tweets_recollected_1704_\"+keyword+\".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate tweets found!\n",
      "Total number of unique tweets:  45210\n"
     ]
    }
   ],
   "source": [
    "for tweetsDF in finalTweetsDFList:\n",
    "    if (tweetsDF.shape[0] != tweetsDF['id'].nunique()):\n",
    "        print(\"Duplicate tweets found!\")\n",
    "print(\"Total number of unique tweets: \", pd.concat(tweetsDFList)['id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [02:33<00:00, 21.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44450 unique tweets written to .txt\n"
     ]
    }
   ],
   "source": [
    "stopWordsSet = set(stopwords.words('english')) \n",
    "stopWordsSet.add(\"u\")\n",
    "ids = []\n",
    "for tweetsDF in tqdm(finalTweetsDFList):\n",
    "    keyword = tweetsDF[\"keyword\"][0]\n",
    "    with open(\"Pre-Processed Tweets/tweets_text_\"+keyword+\".txt\", \"a+\") as text_file:\n",
    "        for index, row in tweetsDF.iterrows():\n",
    "            if (row['id'] not in ids):\n",
    "                ids.append(row['id'])\n",
    "                tweetText = str.strip(re.sub('\\s?https?://.*?[\\s|\\n]', '', row[\"full_text\"]+\" \"))\n",
    "                tweetText = str.strip(re.sub('@.*?[\\s|\\n]', '', tweetText+\" \"))\n",
    "                tweetText = re.sub('&amp;', '', tweetText)\n",
    "                tweetText = \" \".join(re.findall(\"[a-zA-Z]+\",tweetText))\n",
    "                tokenizedText = word_tokenize(tweetText.lower())\n",
    "                finalText = ''.join([WordNetLemmatizer().lemmatize(w) + \" \" for w in tokenizedText if not w in stopWordsSet and len(w) >= 3])\n",
    "                text_file.write(str.strip(finalText) + \"\\n\")\n",
    "print(str(len(ids))+\" unique tweets written to .txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
