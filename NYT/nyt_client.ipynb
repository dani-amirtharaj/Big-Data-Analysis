{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Navaneeth\n",
      "[nltk_data]     kishore\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Navaneeth\n",
      "[nltk_data]     kishore\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Navaneeth\n",
      "[nltk_data]     kishore\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_pt = \"https://api.nytimes.com/svc/search/v2/articlesearch.json\"\n",
    "api_key = \"zYJsAQ968WPrEC01QcoXCUUkn71n9XKI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"music\"\n",
    "subtopics = [\"pop\",\"jazz\",\"classical music\",\"rock\",\"rap\",\"country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = set(stopwords.words(\"english\"))\n",
    "article_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE WITH music\n"
     ]
    }
   ],
   "source": [
    "begin_date = \"20190101\"\n",
    "end_date = \"20190415\"\n",
    "fl = \"web_url, _id\"\n",
    "fq = \"subsection_name:\\\"\" + topic + \"\\\" OR subject: \\\"\" + topic + \"\\\"\"\n",
    "q = topic\n",
    "q_params = {'q':q, 'api-key':api_key, 'fl':fl, 'fq':fq, 'begin_date':begin_date, 'end_date':end_date}\n",
    "res = requests.get(url = access_pt, params = q_params)\n",
    "json_res = res.json()\n",
    "num_hits = json_res['response']['meta']['hits']\n",
    "response_docs = json_res['response']['docs']\n",
    "\n",
    "with open(topic + \".txt\", \"a+\", encoding=\"utf-8\") as text_file:\n",
    "    for a in response_docs:\n",
    "        _id = a['_id']\n",
    "        url = a['web_url']\n",
    "        if _id not in article_dict and url != \"\":\n",
    "            url_res = requests.get(url)\n",
    "            doc = BeautifulSoup(url_res.text, \"html.parser\")\n",
    "            article = \"\"\n",
    "            act_article = \"\"\n",
    "            for div in doc.find_all(\"div\", class_=\"StoryBodyCompanionColumn\"):\n",
    "                for paragraph in div.find_all('p'):\n",
    "                    act_article = act_article + paragraph.get_text() + \" \"\n",
    "                    paragraph = \" \".join(re.findall(\"[a-zA-Z]+\",paragraph.get_text().lower()))\n",
    "                    paragraph_text = word_tokenize(paragraph)\n",
    "                    final_text = ''.join([WordNetLemmatizer().lemmatize(w) + \" \" for w in paragraph_text if not w in stop_list])\n",
    "                    article = article + final_text + \"\\n\"\n",
    "            article_dict[_id] = [url, act_article]\n",
    "            text_file.write(article)\n",
    "\n",
    "num_pages = math.ceil(num_hits/10)\n",
    "page = 0\n",
    "\n",
    "with open(topic + \".txt\", \"a+\", encoding=\"utf-8\") as text_file:\n",
    "    while(num_pages > page):\n",
    "        q_params = {'q':q, 'api-key':api_key, 'fl':fl, 'fq':fq, 'page':page, 'begin_date':begin_date, 'end_date':end_date}\n",
    "        local_res = requests.get(url = access_pt, params = q_params)\n",
    "        local_json_res = local_res.json()\n",
    "        local_response_docs = local_json_res['response']['docs']\n",
    "        for a in local_response_docs:\n",
    "            _id = a['_id']\n",
    "            url = a['web_url']\n",
    "            if _id not in article_dict and url != \"\":\n",
    "                url_res = requests.get(url)\n",
    "                doc = BeautifulSoup(url_res.text, \"html.parser\")\n",
    "                article = \"\"\n",
    "                act_article = \"\"\n",
    "                for div in doc.find_all(\"div\", class_=\"StoryBodyCompanionColumn\"):\n",
    "                    for paragraph in div.find_all('p'):\n",
    "                        act_article = act_article + paragraph.get_text() + \" \"\n",
    "                        paragraph = \" \".join(re.findall(\"[a-zA-Z]+\", paragraph.get_text().lower()))\n",
    "                        paragraph_text = word_tokenize(paragraph)\n",
    "                        final_text = ''.join([WordNetLemmatizer().lemmatize(w) + \" \" for w in paragraph_text if not w in stop_list])\n",
    "                        article = article + final_text + \"\\n\"\n",
    "                article_dict[_id] = [url, act_article]\n",
    "                text_file.write(article)\n",
    "        time.sleep(6)\n",
    "        page += 1\n",
    "\n",
    "print(\"DONE WITH \" + topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE WITH SUBTOPIC pop\n",
      "DONE WITH SUBTOPIC jazz\n",
      "DONE WITH SUBTOPIC classical music\n",
      "DONE WITH SUBTOPIC rock\n",
      "DONE WITH SUBTOPIC rap\n",
      "DONE WITH SUBTOPIC country\n"
     ]
    }
   ],
   "source": [
    "for q in subtopics:\n",
    "    begin_date = \"20190101\"\n",
    "    end_date = \"20190415\"\n",
    "    fl = \"web_url, _id\"\n",
    "    fq = \"subsection_name :\\\"\" + topic + \"\\\" OR subject : \\\"\" + topic + \"\\\" OR subject : \\\"\" + q + \"\\\"\"\n",
    "    q_params = {'q':q, 'api-key':api_key, 'fl':fl, 'fq':fq, 'begin_date':begin_date, 'end_date':end_date}\n",
    "    res = requests.get(url = access_pt, params = q_params)\n",
    "    json_res = res.json()\n",
    "    num_hits = json_res['response']['meta']['hits']\n",
    "    response_docs = json_res['response']['docs']\n",
    "    q_dict = {}\n",
    "\n",
    "    with open(q + \".txt\", \"a+\", encoding=\"utf-8\") as text_file:\n",
    "        for a in response_docs:\n",
    "            _id = a['_id']\n",
    "            url = a['web_url']\n",
    "            if _id not in q_dict and url != \"\":\n",
    "                url_res = requests.get(url)\n",
    "                doc = BeautifulSoup(url_res.text, \"html.parser\")\n",
    "                article = \"\"\n",
    "                act_article = \"\"\n",
    "                for div in doc.find_all(\"div\", class_=\"StoryBodyCompanionColumn\"):\n",
    "                    for paragraph in div.find_all('p'):\n",
    "                        act_article = act_article + paragraph.get_text() + \" \"\n",
    "                        paragraph = \" \".join(re.findall(\"[a-zA-Z]+\",paragraph.get_text().lower()))\n",
    "                        paragraph_text = word_tokenize(paragraph)\n",
    "                        final_text = ''.join([WordNetLemmatizer().lemmatize(w) + \" \" for w in paragraph_text if not w in stop_list])\n",
    "                        article = article + final_text + \"\\n\"\n",
    "                q_dict[_id] = [url, act_article]\n",
    "                text_file.write(article)\n",
    "                if _id not in article_dict:\n",
    "                    article_dict[_id] = [url, act_article]\n",
    "                \n",
    "\n",
    "    num_pages = math.ceil(num_hits/10)\n",
    "    page = 0\n",
    "    \n",
    "    with open(q + \".txt\", \"a+\", encoding=\"utf-8\") as text_file:\n",
    "        while(num_pages > page):\n",
    "            q_params = {'q':q, 'api-key':api_key, 'fl':fl, 'fq':fq, 'page':page, 'begin_date':begin_date, 'end_date':end_date}\n",
    "            local_res = requests.get(url = access_pt, params = q_params)\n",
    "            local_json_res = local_res.json()\n",
    "            local_response_docs = local_json_res['response']['docs']\n",
    "            for a in local_response_docs:\n",
    "                _id = a['_id']\n",
    "                url = a['web_url']\n",
    "                if _id not in q_dict and url != \"\":\n",
    "                    url_res = requests.get(url)\n",
    "                    doc = BeautifulSoup(url_res.text, \"html.parser\")                \n",
    "                    article = \"\"\n",
    "                    act_article = \"\"\n",
    "                    for div in doc.find_all(\"div\", class_=\"StoryBodyCompanionColumn\"):\n",
    "                        for paragraph in div.find_all('p'):\n",
    "                            act_article = act_article + paragraph.get_text() + \" \"\n",
    "                            paragraph = \" \".join(re.findall(\"[a-zA-Z]+\",paragraph.get_text().lower()))\n",
    "                            paragraph_text = word_tokenize(paragraph)\n",
    "                            final_text = ''.join([WordNetLemmatizer().lemmatize(w) + \" \" for w in paragraph_text if not w in stop_list])\n",
    "                            article = article + final_text + \"\\n\"\n",
    "                    q_dict[_id] = [url, act_article]\n",
    "                    text_file.write(article)\n",
    "                    if _id not in article_dict:\n",
    "                        article_dict[_id] = [url, act_article]\n",
    "            time.sleep(6)\n",
    "            page += 1\n",
    "    \n",
    "    qdf = pd.DataFrame.from_dict(q_dict, orient=\"index\")\n",
    "    qdf.to_csv(q+\".csv\")\n",
    "    print(\"DONE WITH SUBTOPIC \" + q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(article_dict, orient=\"index\")\n",
    "df.to_csv(\"articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
